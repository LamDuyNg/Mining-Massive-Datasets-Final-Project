{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP+GbEcMann7lLJCjCUg6vj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LamDuyNg/Mining-Massive-Datasets-Final-Project/blob/main/task2_svd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5Q6g3u7lVNW",
        "outputId": "d7e07b26-901d-4461-be51-ae41900fb092"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, monotonically_increasing_id, size, to_date\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.sql.functions import col, udf, count, sum\n",
        "from pyspark.sql.types import IntegerType, FloatType, ArrayType, DoubleType\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.classification import MultilayerPerceptronClassifier, RandomForestClassifier, LinearSVC, OneVsRest\n",
        "from pyspark.ml.evaluation import RegressionEvaluator, MulticlassClassificationEvaluator\n",
        "from pyspark.sql.window import Window\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql import Row\n",
        "from pyspark.mllib.linalg.distributed import RowMatrix\n",
        "from pyspark.mllib.linalg import DenseMatrix, Vectors\n",
        "from pyspark.sql.functions import udf\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import shutil\n",
        "import glob"
      ],
      "metadata": {
        "id": "b4UeqhJqlWbB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SVD:\n",
        "  def __init__(self, spark, filePath):\n",
        "    self.spark=spark\n",
        "    self.filePath=filePath\n",
        "    self.k=64\n",
        "\n",
        "  def read_file(self):\n",
        "    return self.spark.read.csv(self.filePath, header=False, inferSchema=True)\n",
        "\n",
        "  def seperate_label(self,data):\n",
        "    pixels = data.drop('_c0')  # Extract labels from the first column\n",
        "    labels = data.select('_c0') # Extract pixel values from the remaining\n",
        "    return labels, pixels\n",
        "\n",
        "  def preprocess_data(self, data):\n",
        "    assembler = VectorAssembler(inputCols=data.columns[1:], outputCol=\"features\")\n",
        "    assembled_data = assembler.transform(data)\n",
        "    return assembled_data.select(\"features\", \"_c0\").withColumnRenamed(\"_c0\", \"label\")\n",
        "\n",
        "  def export_cvs(self,data):\n",
        "    data.coalesce(1).write.options(header='False', delimiter=',') \\\n",
        "              .csv(\"result\")\n",
        "\n",
        "    if(os.path.basename(self.filePath)==\"cifar10-test-1k.csv\"):\n",
        "      # Move all CSV files from /content/result to /content/cifar10-test-svd.csv\n",
        "      csv_files = glob.glob('result/*.csv')\n",
        "      for csv_file in csv_files:\n",
        "          shutil.move(csv_file, 'cifar10-test-svd.csv')\n",
        "      # Remove the /content/result directory\n",
        "      shutil.rmtree('result')\n",
        "\n",
        "    if(os.path.basename(self.filePath)==\"cifar10-train-5k.csv\"):\n",
        "      # Move all CSV files from /content/result to /content/cifar10-test-svd.csv\n",
        "      csv_files = glob.glob('result/*.csv')\n",
        "      for csv_file in csv_files:\n",
        "          shutil.move(csv_file, 'cifar10-train-svd.csv')\n",
        "      # Remove the /content/result directory\n",
        "      shutil.rmtree('result')\n",
        "\n",
        "\n",
        "  def run_svd(self):\n",
        "    data=self.read_file()\n",
        "\n",
        "\n",
        "    data=self.preprocess_data(data)\n",
        "    ml_vectors = data.select(\"features\").rdd.map(lambda row: Vectors.dense(row.features.toArray()))\n",
        "\n",
        "    row=RowMatrix(ml_vectors)\n",
        "    self.svd = row.computeSVD(self.k, True)\n",
        "\n",
        "    self.U = self.svd.U #RowMatrix\n",
        "    self.S = DenseMatrix(len(self.svd.s), len(self.svd.s), np.diag(self.svd.s).ravel(\"F\")) #dense vector\n",
        "\n",
        "    # Convert RowMatrix to DataFrame\n",
        "    def row_to_dict(row):\n",
        "      return Row(features=Vectors.dense(row.toArray()))\n",
        "\n",
        "    row_matrix_rows = self.U.multiply(self.S).rows.map(row_to_dict)\n",
        "    df = self.spark.createDataFrame(row_matrix_rows)\n",
        "\n",
        "    # Define a UDF to convert a dense vector to an array of doubles\n",
        "    vector_to_array = udf(lambda v: v.toArray().tolist(), ArrayType(DoubleType()))\n",
        "\n",
        "    # Apply the UDF to the 'features' column\n",
        "    df = df.withColumn(\"features_array\", vector_to_array(df[\"features\"]))\n",
        "\n",
        "    # Explode the array into multiple columns\n",
        "    df = df.select([df[\"features_array\"][i].alias(f\"_c{i+1}\") for i in range(self.k)])\n",
        "\n",
        "    data = data.withColumn(\"unique_id\", monotonically_increasing_id())\n",
        "    df = df.withColumn(\"unique_id\", monotonically_increasing_id())\n",
        "    # Perform a join based on the unique identifier\n",
        "    merged_df = data[['label','unique_id']].join(df, \"unique_id\").drop(\"unique_id\")\n",
        "\n",
        "    self.export_cvs(merged_df)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    spark = SparkSession.builder.appName(\"SVD\").getOrCreate()\n",
        "\n",
        "\n",
        "    train_file = \"cifar10-train-5k.csv\"\n",
        "    svd=SVD(spark, train_file)\n",
        "    svd.run_svd()\n",
        "\n",
        "    test_file = \"cifar10-test-1k.csv\"\n",
        "    svd=SVD(spark, test_file)\n",
        "    svd.run_svd()\n",
        "\n",
        "    spark.stop()"
      ],
      "metadata": {
        "id": "gxjliH4DlXd-"
      },
      "execution_count": 4,
      "outputs": []
    }
  ]
}